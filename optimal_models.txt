================================================================================
GLOBAL LAYER/ACTIVATION DEFAULTS
================================================================================
• LSTM layers (Keras):
  - activation='tanh'                 # state update nonlinearity
  - recurrent_activation='sigmoid'    # gate nonlinearity
  - rationale: the default (tanh/sigmoid) is typically best for stability in sequence modeling.
• Dense hidden layers (if any): 
  - activation='relu'  (robust general-purpose)
  - alternatives to try: 'swish' (a.k.a. 'silu'), 'gelu', or 'selu' (with AlphaDropout).
• Output layer (regression/forecasting):
  - activation='linear'  (recommended default when target is real-valued, e.g., log1p-transformed counts)
  - If you DO NOT log-transform positive targets and need strictly non-negative outputs, consider:
       activation='softplus'  or activation='relu'
    (softplus is smoother than relu).

Tip on targets:
• For count-like targets, prefer training on log1p(y) with output activation='linear' and invert with expm1 at inference.
• For raw positive-only targets without log transform, prefer output activation='softplus'.

================================================================================
CALLBACKS/OPTIMIZER (unchanged)
================================================================================
EarlyStopping(patience=12), ReduceLROnPlateau(factor=0.5, patience=6), ModelCheckpoint.
Optimizer: AdamW or Adam with the learning rates previously specified.

================================================================================
DATASET-SPECIFIC MODELS (with activations)
================================================================================

[merged_data]  (multivariate with exogenous features)
Recommended (stacked) LSTM:
  LSTM(64, return_sequences=True, activation='tanh', recurrent_activation='sigmoid')
  Dropout(0.30)
  LSTM(32, activation='tanh', recurrent_activation='sigmoid')
  Dropout(0.30)
  Dense(1, activation='linear')    # or 'softplus' if you keep targets un-transformed and non-negative

Tuning grid additions:
  • Try Dense head with activation ∈ {'linear','softplus'} depending on target transform.
  • If adding a Dense hidden before the head: Dense(32, activation ∈ {'relu','swish'}).

[merged_no_weather]  (leaner feature set)
Recommended LSTM:
  LSTM(48, return_sequences=True, activation='tanh', recurrent_activation='sigmoid')
  Dropout(0.25)
  LSTM(24, activation='tanh', recurrent_activation='sigmoid')
  Dropout(0.25)
  Dense(1, activation='linear')    # or 'softplus' if outputs must be ≥ 0 without log transform

Optional Dense hidden:
  Dense(32, activation='relu') before the final Dense can help if features are moderately complex.

[trend_counts_over_time]  (mostly univariate counts)
Recommended compact LSTM:
  LSTM(32, activation='tanh', recurrent_activation='sigmoid')
  Dropout(0.20)
  Dense(1, activation='linear')    # preferred with log1p targets
  # If training on raw counts (no log1p): use Dense(1, activation='softplus').

Additional notes for this dataset:
  • Strongly consider log1p transform → linear output; it improves stability and error symmetry.
  • If variance is very low, also test Dense(1, activation='relu') as a simple non-negativity constraint.

================================================================================
FINE-TUNING PLAYBOOK (activation-specific tweaks)
================================================================================
1) If gradients vanish/learning is slow:
   - Slightly increase learning rate or insert a Dense(32, activation='relu'/'swish') between LSTM and head.
2) If validation loss oscillates or spikes:
   - Keep LSTM activations at default (tanh/sigmoid), lower lr, and add LayerNormalization (between LSTM and Dense).
3) If predictions go negative but the target shouldn’t:
   - Use 'softplus' on the output OR train on log1p with linear output.
4) If model underfits:
   - Add a Dense(64, activation='relu'/'swish') before the head; or increase first LSTM units.
5) If overfits:
   - Increase Dropout(Δ=0.05–0.1), reduce Dense hidden size, or add L2 kernel_regularizer to Dense/LSTM.
